{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSIDE = 8\n",
    "NUMPIX = 12 * NSIDE ** 2\n",
    "DEPTH = 4\n",
    "SHOW_IMAGES = True\n",
    "\n",
    "RECT = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "\n",
    "def make_dir():\n",
    "    image_dir = 'Saved_Images'\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "def save_img(img, name):\n",
    "    img = img.view(img.size(0), 1, 12 * NSIDE // 8, 64 * NSIDE // 8)\n",
    "    save_image(img, name)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "base = torch.float32\n",
    "\n",
    "#pin to gpu\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import set_seed\n",
    "set_seed(2021)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pickle\n",
    "# import numpy as np\n",
    "\n",
    "# from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# class HealpixDataLoader():\n",
    "\n",
    "#     def __init__(self, file_name, train_pct, val_pct, base, device, output_dim, BATCH_SIZE, SHUFFLE):\n",
    "#         # Load in dataset and split into x, y categories\n",
    "#         x, y = self.load_data(file_name)\n",
    "\n",
    "#         # Reshape output data into (depth by width by length)\n",
    "#         x, y = self.reshape_data(x, y, output_dim)\n",
    "\n",
    "#         # Split into train, test, and validation arrays\n",
    "#         x_train, x_val, x_test, y_train, y_val, y_test = self.split_data(x, y, train_pct, val_pct)\n",
    "\n",
    "#         # Create TensorDatasets\n",
    "#         train_set, val_set, test_set = self.create_datasets(x_train, x_val, x_test, y_train, y_val, y_test, base, device)\n",
    "\n",
    "#         # Create DataLoaders\n",
    "#         train_loader, val_loader, test_loader = self.create_loaders(train_set, val_set, test_set, BATCH_SIZE, SHUFFLE)\n",
    "\n",
    "#         self.train_loader = train_loader\n",
    "#         self.val_loader = val_loader\n",
    "#         self.test_loader = test_loader\n",
    "\n",
    "#     def load_data(self, file_name):\n",
    "#         file_name = f\"split_sphere_datasets_NSIDE{NSIDE}.pkl\"\n",
    "\n",
    "#         f = open(file_name, \"rb\")\n",
    "#         dataset = pickle.load(f)\n",
    "#         f.close()\n",
    "\n",
    "#         x_combined = []\n",
    "#         y_combined = []\n",
    "\n",
    "#         for i in range(len(dataset)):\n",
    "#             x = dataset[i]['data']\n",
    "#             y = dataset[i]['label']\n",
    "\n",
    "#             x_combined.append(x)\n",
    "#             y_combined.append(y)\n",
    "            \n",
    "#         x_combined = np.array(x_combined)\n",
    "#         y_combined = np.array(y_combined)\n",
    "\n",
    "#         return x_combined, y_combined\n",
    "    \n",
    "#     def reshape_data(self, x, y, output_dim):\n",
    "#         depth, width, length = output_dim\n",
    "#         y = y.reshape((len(y), depth, width, length))\n",
    "\n",
    "#         return x, y\n",
    "\n",
    "#     def split_data(self, x, y, train_pct, val_pct):\n",
    "#         train_len = int(len(x) * train_pct)\n",
    "#         val_len = int(len(x) * val_pct)\n",
    "\n",
    "#         x_train = x[:train_len]\n",
    "#         x_val = x[train_len : train_len + val_len]\n",
    "#         x_test = x[train_len + val_len:]\n",
    "\n",
    "#         y_train = y[:train_len]\n",
    "#         y_val = y[train_len : train_len + val_len]\n",
    "#         y_test = y[train_len + val_len:]\n",
    "\n",
    "#         return x_train, x_val, x_test, y_train, y_val, y_test\n",
    "    \n",
    "#     def create_datasets(self, x_train, x_val, x_test, y_train, y_val, y_test, base, device):\n",
    "#         # Need to convert bases based on device \n",
    "#         train_set = TensorDataset(torch.tensor(x_train).to(dtype=base, device=device), \n",
    "#                                   torch.tensor(y_train).to(dtype=base, device=device))\n",
    "#         val_set = TensorDataset(torch.tensor(x_val).to(dtype=base, device=device), \n",
    "#                                 torch.tensor(y_val).to(dtype=base, device=device))\n",
    "#         test_set = TensorDataset(torch.tensor(x_test).to(dtype=base, device=device), \n",
    "#                                  torch.tensor(y_test).to(dtype=base, device=device))\n",
    "        \n",
    "#         return train_set, val_set, test_set\n",
    "\n",
    "#     def create_loaders(self, train_set, val_set, test_set, BATCH_SIZE, SHUFFLE=True):\n",
    "#         train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=SHUFFLE)\n",
    "#         val_loader = DataLoader(val_set, BATCH_SIZE, shuffle=SHUFFLE)\n",
    "#         test_loader = DataLoader(test_set, BATCH_SIZE, shuffle=SHUFFLE)\n",
    "\n",
    "#         return train_loader, val_loader, test_loader\n",
    "\n",
    "#     def get_loaders(self):\n",
    "#         return self.train_loader, self.val_loader, self.test_loader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = f\"split_sphere_datasets_NSIDE{NSIDE}.pkl\"\n",
    "\n",
    "\n",
    "# length = 32 * NSIDE // 8\n",
    "# width = 24 * NSIDE // 8\n",
    "# depth = 4\n",
    "# BATCH_SIZE = 16\n",
    "# SHUFFLE = True\n",
    "\n",
    "# output_dim = (depth, width, length)\n",
    "\n",
    "# healpix_loader = HealpixDataLoader(file_name, 0.7, 0.15, base, device, output_dim, BATCH_SIZE, SHUFFLE)\n",
    "\n",
    "# train_loader, val_loader, test_loader = healpix_loader.get_loaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Open the file.\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "import healpy as hp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(file_name):\n",
    "    file_name = f\"split_sphere_datasets_NSIDE{NSIDE}.pkl\"\n",
    "\n",
    "    f = open(file_name, \"rb\")\n",
    "    dataset = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "file_name = f\"split_sphere_datasets_NSIDE{NSIDE}.pkl\"\n",
    "dataset = load_data(file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Input and Output Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_combined shape:  (2048, 2)\n",
      "y_combined shape:  (2048, 3072)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Create the training and validation datasets. Do any necessary reshaping. \n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_combined = []\n",
    "y_combined = []\n",
    "\n",
    "\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    x = dataset[i]['data']\n",
    "    y = dataset[i]['label']\n",
    "\n",
    "    x_combined.append(x)\n",
    "    y_combined.append(y)\n",
    "    \n",
    "x_combined = np.array(x_combined)\n",
    "y_combined = np.array(y_combined)\n",
    "\n",
    "print(\"x_combined shape: \", x_combined.shape)\n",
    "print(\"y_combined shape: \", y_combined.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split into Training, Val, and Test. Also Reshape into Rectangle if Specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (1536, 2)\n",
      "x_val shape:  (256, 2)\n",
      "x_test shape:  (256, 2)\n",
      "\n",
      "y_train shape:  (1536, 4, 24, 32)\n",
      "y_val shape:  (256, 4, 24, 32)\n",
      "y_test shape:  (256, 4, 24, 32)\n"
     ]
    }
   ],
   "source": [
    "# train_len = int(0.7 * len(y_combined)) # 70%\n",
    "# val_len = int(0.15 * len(y_combined)) # 15%\n",
    "# test_len = len(y_combined) - train_len - val_len # 15%\n",
    "\n",
    "train_len = 1536\n",
    "val_len = 256\n",
    "test_len = 256\n",
    "\n",
    "\n",
    "x_train = x_combined[:train_len]\n",
    "x_val = x_combined[train_len : train_len + val_len]\n",
    "x_test = x_combined[train_len + val_len:]\n",
    "\n",
    "y_train = y_combined[:train_len]\n",
    "y_val = y_combined[train_len : train_len + val_len]\n",
    "y_test = y_combined[train_len + val_len:]\n",
    "\n",
    "if RECT:\n",
    "    # NOTE: Can change based on input size of data\n",
    "    # For now, default to 12, 64 size for NSIDE 8 that is scaled up for larger NSIDEs. \n",
    "    length = 32 * NSIDE // 8\n",
    "    width = 24 * NSIDE // 8\n",
    "\n",
    "    y_train = y_train.reshape((len(y_train), DEPTH, width, length))\n",
    "    y_val = y_val.reshape((len(y_val), DEPTH, width, length))\n",
    "    y_test = y_test.reshape((len(y_test), DEPTH, width, length))\n",
    "\n",
    "\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_val shape: \", x_val.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "print()\n",
    "\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_val shape: \", y_val.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "NUM_EPOCHS = 1000\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 32 # previously 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "# train_set = TensorDataset(torch.tensor(x_train).to(dtype=base, device=device), torch.tensor(y_train).to(dtype=base, device=device))\n",
    "# val_set = TensorDataset(torch.tensor(x_val).to(dtype=base, device=device), torch.tensor(y_val).to(dtype=base, device=device))\n",
    "# test_set = TensorDataset(torch.tensor(x_test).to(dtype=base, device=device), torch.tensor(y_test).to(dtype=base, device=device))\n",
    "\n",
    "# train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
    "# val_loader = DataLoader(val_set, BATCH_SIZE, shuffle=True)\n",
    "# test_loader = DataLoader(test_set, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv2d, Sequential, ConvTranspose2d, ReLU, MaxPool2d, Linear, Conv3d, Tanh, Dropout\n",
    "\n",
    "\n",
    "class ConvExpand(Module): \n",
    "    def __init__(self, linear_layers, conv_layers, mid_rect_size, batch_size, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Put encoder layers in Sequential container\n",
    "        # First increase from 1 --> 64 channels\n",
    "        # Keep decreasing number of channels\n",
    "        \n",
    "        self.linear_layers = linear_layers\n",
    "        self.conv_layers = conv_layers\n",
    "\n",
    "        self.mid_rect_size = mid_rect_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    # Run x through each layer\n",
    "    def forward(self, x):\n",
    "        for layer in self.linear_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "\n",
    "        # IMPORTANT: Reshaping output of linear layer into rect form\n",
    "        # Need to reshape into (BATCH SIZE, NUM_CHANNELS, ... )\n",
    "        # usually batch size is first parameter, but sometimes if smaller batch then just use whatever x has\n",
    "        # originally self.batch_size as first param, but now this\n",
    "        x = x.view(x.size(0), 4, self.mid_rect_size[0], self.mid_rect_size[1])\n",
    "        \n",
    "        for layer in self.conv_layers:\n",
    "            # print(x.shape)\n",
    "            x = layer(x)\n",
    "        \n",
    "        # print(x.detach().numpy().shape)\n",
    "        # print()\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fcmodel8 = Sequential(\n",
    "           Linear(2, 12),\n",
    "           ReLU(),\n",
    "           Linear(12, 384),\n",
    "           ReLU(),\n",
    "           Linear(384, 1536),\n",
    "           ReLU(),\n",
    "           Linear(1536, 3072)\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: change linear layer output to batch size * 256 so dimensions match? hmm\n",
    "lin8 = Sequential (\n",
    "   Linear(2, 12),\n",
    "   ReLU(),\n",
    "   Linear(12, 384),\n",
    "   ReLU(),\n",
    "   Linear(384, 3072),\n",
    ")\n",
    "\n",
    "\n",
    "mid_rect_size = (width, length)\n",
    "\n",
    "\n",
    "conv8 = Sequential (\n",
    "   Conv2d(4, 16, kernel_size=3, padding=1, padding_mode=\"reflect\"),\n",
    "   ReLU(),\n",
    "   Conv2d(16, 4, kernel_size=3, padding=1, padding_mode=\"reflect\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "expand8 = ConvExpand(lin8, conv8, mid_rect_size, BATCH_SIZE, NUM_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = expand8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# Use MSE Loss\n",
    "# need to specify cpu\n",
    "criterion = MSELoss().to(dtype=base, device=device)\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate performance of model on some data\n",
    "def eval_performance(model, data_loader):\n",
    "    # Set model to evaluation mode to conserve memory \n",
    "    model.eval()\n",
    "\n",
    "    # Don't want to waste memory on gradients\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for x, y in data_loader:\n",
    "            pred = model(x)\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            # CRUCIAL: need to multiply by batch size since loss.item() will give total loss / batch_size\n",
    "            running_loss += loss.item() * pred.shape[0]\n",
    "            \n",
    "        \n",
    "        # Revert back to train mode\n",
    "        model.train()\n",
    "\n",
    "        # IMPORTANT: need to do train_loader.dataset to get total # training examples instead of # batches\n",
    "        # len(train_loader) would just give the # of batches\n",
    "        return running_loss / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "epoch = 0\n",
    "\n",
    "def train(model, train_loader, NUM_EPOCHS):\n",
    "    global train_loss_hist\n",
    "    global val_loss_hist\n",
    "    global epoch\n",
    "\n",
    "    lr_changed = False\n",
    "    lr_change2 = False\n",
    "\n",
    "    smallest_val_loss = 10000\n",
    "\n",
    "    while epoch < NUM_EPOCHS:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Go through each batch of the data (can pass in entire batch at once!)\n",
    "        # batch - number of training examples for one forward/backward pass. So pass in batch data values then update weights. \n",
    "        for x_vals, y_vals in train_loader:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Remember, it's in batches. \n",
    "            \n",
    "            '''\n",
    "\n",
    "            Reset gradients to 0 so updating of weights can be done correctly.\n",
    "\n",
    "            When we do loss.backward(), gradients are calculated. Then, optimizer.step() does gradient descent.\n",
    "            For the next batch, we don't want these gradients to still be lingering (because a new input will have new gradients).\n",
    "            Thus, we have to reset the gradients to 0. \n",
    "\n",
    "            NOTE: This is not the same as setting the weights to 0! We are just resetting the calculated gradients.\n",
    "            \n",
    "            '''\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate model outputs\n",
    "            outputs = model(x_vals)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, y_vals)\n",
    "\n",
    "            # Calculate gradients \n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent to update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # CRUCIAL: need to multiply by batch size since loss.item() will give total loss / batch_size\n",
    "            running_loss += loss.item() * outputs.shape[0]\n",
    "\n",
    "\n",
    "        # IMPORTANT: need to do train_loader.dataset to get total # training examples instead of # batches\n",
    "        # len(train_loader) would just give the # of batches\n",
    "        loss = running_loss / len(train_loader.dataset)\n",
    "        train_loss_hist.append(loss)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        # change learning rate when loss < 0.01\n",
    "        if loss < 0.1 and not lr_changed:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = 0.005\n",
    "            \n",
    "            lr_changed = True\n",
    "        \n",
    "        if loss < 0.035 and not lr_change2:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = 0.0008\n",
    "                \n",
    "            lr_change2 = True\n",
    "\n",
    "\n",
    "        val_loss = eval_performance(model, val_loader)\n",
    "        val_loss_hist.append(val_loss)\n",
    "\n",
    "        smallest_val_loss = min(smallest_val_loss, val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1} of {NUM_EPOCHS}, Train Loss: {loss}, Val Loss: {val_loss}')\n",
    "        # print(f'Epoch {epoch + 1} of {NUM_EPOCHS}, Train Loss: {loss}')\n",
    "\n",
    "\n",
    "        patience = 30\n",
    "        # Decide if early stopping necessary \n",
    "        recent_min = min(val_loss_hist[-patience:])\n",
    "        if smallest_val_loss < recent_min:\n",
    "            print(\"Stopped early\")\n",
    "            return\n",
    "\n",
    "        # # Saving a full batch!\n",
    "        # if epoch % 2 == 0:\n",
    "        #     save_img(y_vals.cpu().data, name='./Cone_Images/truth_batch_{}.png'.format(epoch))\n",
    "        #     save_img(outputs.cpu().data, name='./Cone_Images/predicted_batch_{}.png'.format(epoch))\n",
    "        \n",
    "        epoch += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/94j8y0qs6r76hpyncbj21th40000gn/T/ipykernel_30671/3655824926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/h7/94j8y0qs6r76hpyncbj21th40000gn/T/ipykernel_30671/3455131070.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mval_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_performance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mval_loss_hist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h7/94j8y0qs6r76hpyncbj21th40000gn/T/ipykernel_30671/3549156459.py\u001b[0m in \u001b[0;36meval_performance\u001b[0;34m(model, data_loader)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn_response/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h7/94j8y0qs6r76hpyncbj21th40000gn/T/ipykernel_30671/2786339656.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn_response/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn_response/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, NUM_EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"training loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"train loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(train_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"val loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(val_loss_hist[:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_sample(model, data_loader):\n",
    "    for x, y in data_loader:\n",
    "        # Set model to evaluation mode to conserve memory \n",
    "        model.eval()\n",
    "\n",
    "        # Don't want to waste memory on gradients\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = model(x)\n",
    "\n",
    "            truth = torch.reshape(y[0], (DEPTH, NUMPIX, ))[3].detach().numpy()\n",
    "            model_pred = np.reshape(pred[0].detach().numpy(), (DEPTH, NUMPIX, ))[3]\n",
    "\n",
    "            diff = np.absolute(model_pred - truth)\n",
    "            \n",
    "            # [0] because 1 channel so need to go inside\n",
    "            hp.mollview(truth, title=\"Ground Truth\", nlocs=5)\n",
    "            hp.mollview(model_pred, title=\"Model Prediction\", nlocs=5)\n",
    "            hp.mollview(diff, title=\"Difference Map\", nlocs=5)\n",
    "\n",
    "            print(np.mean(diff))\n",
    "\n",
    "            # set back to train mode\n",
    "            model.train()\n",
    "            break\n",
    "    \n",
    "if SHOW_IMAGES:\n",
    "    display_sample(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Val loss: \" , eval_performance(model, val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader of Model Outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_outputs(model, path):\n",
    "    preds = []\n",
    "    truths = []\n",
    "\n",
    "    for x, y in val_loader:\n",
    "    # Set model to evaluation mode to conserve memory \n",
    "        model.eval()\n",
    "\n",
    "        # Don't want to waste memory on gradients\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = model(x)\n",
    "\n",
    "            pred_np = pred.detach().numpy()\n",
    "            truth_np = y.detach().numpy()\n",
    "\n",
    "            new_shape = (4, 12, 64)\n",
    "\n",
    "            # batched right now, so need to unbatch \n",
    "            for i in range(len(pred_np)):\n",
    "\n",
    "                # need to also reshape each element\n",
    "                pred_reshape = np.reshape(pred_np[i], new_shape)\n",
    "                truth_reshape = np.reshape(truth_np[i], new_shape)\n",
    "\n",
    "                print(pred_reshape.shape)\n",
    "\n",
    "                # need to add each depth individually\n",
    "                for depth in range(4):\n",
    "                    # [0] since first 1 dimension\n",
    "                    preds.append(pred_reshape[depth])\n",
    "                    truths.append(truth_reshape[depth])\n",
    "    \n",
    "    preds = np.array(preds)\n",
    "    truths = np.array(truths)\n",
    "\n",
    "    print(preds.shape)\n",
    "            \n",
    "    combined_set = TensorDataset(torch.tensor(preds).to(dtype=base, device=device), torch.tensor(truths).to(dtype=base, device=device))\n",
    "    loader = DataLoader(combined_set, 1, shuffle=True)\n",
    "    \n",
    "    torch.save(loader, path)\n",
    "\n",
    "    # set back to train mode\n",
    "    model.train()\n",
    "\n",
    "final_train_loss = round(float(train_loss_hist[-1]), 3)\n",
    "final_val_loss = round(float(val_loss_hist[-1]), 3)\n",
    "\n",
    "name = f\"saved-pred-outputs/output-loader-model_NSIDE{NSIDE}_trainloss{final_train_loss}_valloss{final_val_loss}_epochs{epoch}.pth\"\n",
    "save_model_outputs(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nn_response')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a63c20d5ea98d5ea615593d44a9fb564882bf3b15df23c63fb54c70ca4209996"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

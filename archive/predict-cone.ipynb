{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "NSIDE = 8\n",
    "NUMPIX = 12 * NSIDE ** 2\n",
    "DEPTH = 4\n",
    "SHOW_IMAGES = True\n",
    "\n",
    "RECT = True"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/akotamraju/opt/anaconda3/envs/nn_response/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from torchvision.utils import save_image\n",
    "import os\n",
    "import torch\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda:0'\n",
    "    else:\n",
    "        device = 'cpu'\n",
    "    return device\n",
    "\n",
    "\n",
    "def make_dir():\n",
    "    image_dir = 'Saved_Images'\n",
    "    if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)\n",
    "def save_img(img, name):\n",
    "    img = img.view(img.size(0), 1, 12 * NSIDE // 8, 64 * NSIDE // 8)\n",
    "    save_image(img, name)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_device()\n",
    "base = torch.float32\n",
    "\n",
    "#pin to gpu\n",
    "device"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'src'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/94j8y0qs6r76hpyncbj21th40000gn/T/ipykernel_30900/2671678520.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2021\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'src'"
     ]
    }
   ],
   "source": [
    "from src.utils import set_seed\n",
    "set_seed(2021)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Open the file.\n",
    "'''\n",
    "\n",
    "import pickle\n",
    "import healpy as hp\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def load_data(file_name):\n",
    "    file_name = f\"split_sphere_datasets_NSIDE{NSIDE}.pkl\"\n",
    "\n",
    "    f = open(file_name, \"rb\")\n",
    "    dataset = pickle.load(f)\n",
    "    f.close()\n",
    "\n",
    "    return dataset\n",
    "\n",
    "file_name = f\"split_sphere_datasets_NSIDE{NSIDE}.pkl\"\n",
    "dataset = load_data(file_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Input and Output Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_combined shape:  (2048, 2)\n",
      "y_combined shape:  (2048, 3072)\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "Create the training and validation datasets. Do any necessary reshaping. \n",
    "\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x_combined = []\n",
    "y_combined = []\n",
    "\n",
    "\n",
    "NUM_CHANNELS = 1\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    x = dataset[i]['data']\n",
    "    y = dataset[i]['label']\n",
    "\n",
    "    x_combined.append(x)\n",
    "    y_combined.append(y)\n",
    "    \n",
    "x_combined = np.array(x_combined)\n",
    "y_combined = np.array(y_combined)\n",
    "\n",
    "print(\"x_combined shape: \", x_combined.shape)\n",
    "print(\"y_combined shape: \", y_combined.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split into Training, Val, and Test. Also Reshape into Rectangle if Specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape:  (1536, 2)\n",
      "x_val shape:  (256, 2)\n",
      "x_test shape:  (256, 2)\n",
      "\n",
      "y_train shape:  (1536, 4, 24, 32)\n",
      "y_val shape:  (256, 4, 24, 32)\n",
      "y_test shape:  (256, 4, 24, 32)\n"
     ]
    }
   ],
   "source": [
    "# train_len = int(0.7 * len(y_combined)) # 70%\n",
    "# val_len = int(0.15 * len(y_combined)) # 15%\n",
    "# test_len = len(y_combined) - train_len - val_len # 15%\n",
    "\n",
    "train_len = 1536\n",
    "val_len = 256\n",
    "test_len = 256\n",
    "\n",
    "\n",
    "x_train = x_combined[:train_len]\n",
    "x_val = x_combined[train_len : train_len + val_len]\n",
    "x_test = x_combined[train_len + val_len:]\n",
    "\n",
    "y_train = y_combined[:train_len]\n",
    "y_val = y_combined[train_len : train_len + val_len]\n",
    "y_test = y_combined[train_len + val_len:]\n",
    "\n",
    "if RECT:\n",
    "    # NOTE: Can change based on input size of data\n",
    "    # For now, default to 12, 64 size for NSIDE 8 that is scaled up for larger NSIDEs. \n",
    "    length = 32 * NSIDE // 8\n",
    "    width = 24 * NSIDE // 8\n",
    "\n",
    "    y_train = y_train.reshape((len(y_train), DEPTH, width, length))\n",
    "    y_val = y_val.reshape((len(y_val), DEPTH, width, length))\n",
    "    y_test = y_test.reshape((len(y_test), DEPTH, width, length))\n",
    "\n",
    "\n",
    "print(\"x_train shape: \", x_train.shape)\n",
    "print(\"x_val shape: \", x_val.shape)\n",
    "print(\"x_test shape: \", x_test.shape)\n",
    "print()\n",
    "\n",
    "print(\"y_train shape: \", y_train.shape)\n",
    "print(\"y_val shape: \", y_val.shape)\n",
    "print(\"y_test shape: \", y_test.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "NUM_EPOCHS = 1000\n",
    "LEARNING_RATE = 0.01\n",
    "BATCH_SIZE = 32 # previously 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "train_set = TensorDataset(torch.tensor(x_train).to(dtype=base, device=device), torch.tensor(y_train).to(dtype=base, device=device))\n",
    "val_set = TensorDataset(torch.tensor(x_val).to(dtype=base, device=device), torch.tensor(y_val).to(dtype=base, device=device))\n",
    "test_set = TensorDataset(torch.tensor(x_test).to(dtype=base, device=device), torch.tensor(y_test).to(dtype=base, device=device))\n",
    "\n",
    "train_loader = DataLoader(train_set, BATCH_SIZE, shuffle=True)\n",
    "val_loader = DataLoader(val_set, BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_set, BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Define Model Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module, Conv2d, Sequential, ConvTranspose2d, ReLU, MaxPool2d, Linear, Conv3d, Tanh, Dropout\n",
    "\n",
    "\n",
    "class ConvExpand(Module): \n",
    "    def __init__(self, linear_layers, conv_layers, mid_rect_size, batch_size, num_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        # Put encoder layers in Sequential container\n",
    "        # First increase from 1 --> 64 channels\n",
    "        # Keep decreasing number of channels\n",
    "        \n",
    "        self.linear_layers = linear_layers\n",
    "        self.conv_layers = conv_layers\n",
    "\n",
    "        self.mid_rect_size = mid_rect_size\n",
    "        self.batch_size = batch_size\n",
    "        self.num_channels = num_channels\n",
    "\n",
    "    # Run x through each layer\n",
    "    def forward(self, x):\n",
    "        for layer in self.linear_layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "\n",
    "        # IMPORTANT: Reshaping output of linear layer into rect form\n",
    "        # Need to reshape into (BATCH SIZE, NUM_CHANNELS, ... )\n",
    "        # usually batch size is first parameter, but sometimes if smaller batch then just use whatever x has\n",
    "        # originally self.batch_size as first param, but now this\n",
    "        x = x.view(x.size(0), 4, self.mid_rect_size[0], self.mid_rect_size[1])\n",
    "        \n",
    "        for layer in self.conv_layers:\n",
    "            # print(x.shape)\n",
    "            x = layer(x)\n",
    "        \n",
    "        # print(x.detach().numpy().shape)\n",
    "        # print()\n",
    "        return x\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fcmodel8 = Sequential(\n",
    "           Linear(2, 12),\n",
    "           ReLU(),\n",
    "           Linear(12, 384),\n",
    "           ReLU(),\n",
    "           Linear(384, 1536),\n",
    "           ReLU(),\n",
    "           Linear(1536, 3072)\n",
    "   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT: change linear layer output to batch size * 256 so dimensions match? hmm\n",
    "lin8 = Sequential (\n",
    "   Linear(2, 12),\n",
    "   ReLU(),\n",
    "   Linear(12, 384),\n",
    "   ReLU(),\n",
    "   Linear(384, 3072),\n",
    ")\n",
    "\n",
    "\n",
    "mid_rect_size = (width, length)\n",
    "\n",
    "\n",
    "conv8 = Sequential (\n",
    "   Conv2d(4, 16, kernel_size=3, padding=1, padding_mode=\"reflect\"),\n",
    "   ReLU(),\n",
    "   Conv2d(16, 4, kernel_size=3, padding=1, padding_mode=\"reflect\"),\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "expand8 = ConvExpand(lin8, conv8, mid_rect_size, BATCH_SIZE, NUM_CHANNELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = expand8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Create Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "# Use MSE Loss\n",
    "# need to specify cpu\n",
    "criterion = MSELoss().to(dtype=base, device=device)\n",
    "\n",
    "# Use Adam optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to evaluate performance of model on some data\n",
    "def eval_performance(model, data_loader):\n",
    "    # Set model to evaluation mode to conserve memory \n",
    "    model.eval()\n",
    "\n",
    "    # Don't want to waste memory on gradients\n",
    "    with torch.no_grad():\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for x, y in data_loader:\n",
    "            pred = model(x)\n",
    "\n",
    "            loss = criterion(pred, y)\n",
    "\n",
    "            # CRUCIAL: need to multiply by batch size since loss.item() will give total loss / batch_size\n",
    "            running_loss += loss.item() * pred.shape[0]\n",
    "            \n",
    "        \n",
    "        # Revert back to train mode\n",
    "        model.train()\n",
    "\n",
    "        # IMPORTANT: need to do train_loader.dataset to get total # training examples instead of # batches\n",
    "        # len(train_loader) would just give the # of batches\n",
    "        return running_loss / len(data_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_loss_hist = []\n",
    "val_loss_hist = []\n",
    "epoch = 0\n",
    "\n",
    "def train(model, train_loader, NUM_EPOCHS):\n",
    "    global train_loss_hist\n",
    "    global val_loss_hist\n",
    "    global epoch\n",
    "\n",
    "    lr_changed = False\n",
    "    lr_change2 = False\n",
    "\n",
    "    smallest_val_loss = 10000\n",
    "\n",
    "    while epoch < NUM_EPOCHS:\n",
    "        running_loss = 0.0\n",
    "\n",
    "        # Go through each batch of the data (can pass in entire batch at once!)\n",
    "        # batch - number of training examples for one forward/backward pass. So pass in batch data values then update weights. \n",
    "        for x_vals, y_vals in train_loader:\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            # Remember, it's in batches. \n",
    "            \n",
    "            '''\n",
    "\n",
    "            Reset gradients to 0 so updating of weights can be done correctly.\n",
    "\n",
    "            When we do loss.backward(), gradients are calculated. Then, optimizer.step() does gradient descent.\n",
    "            For the next batch, we don't want these gradients to still be lingering (because a new input will have new gradients).\n",
    "            Thus, we have to reset the gradients to 0. \n",
    "\n",
    "            NOTE: This is not the same as setting the weights to 0! We are just resetting the calculated gradients.\n",
    "            \n",
    "            '''\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Calculate model outputs\n",
    "            outputs = model(x_vals)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(outputs, y_vals)\n",
    "\n",
    "            # Calculate gradients \n",
    "            loss.backward()\n",
    "\n",
    "            # Do gradient descent to update the weights.\n",
    "            optimizer.step()\n",
    "\n",
    "            # CRUCIAL: need to multiply by batch size since loss.item() will give total loss / batch_size\n",
    "            running_loss += loss.item() * outputs.shape[0]\n",
    "\n",
    "\n",
    "        # IMPORTANT: need to do train_loader.dataset to get total # training examples instead of # batches\n",
    "        # len(train_loader) would just give the # of batches\n",
    "        loss = running_loss / len(train_loader.dataset)\n",
    "        train_loss_hist.append(loss)\n",
    "\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "\n",
    "        # change learning rate when loss < 0.01\n",
    "        if loss < 0.1 and not lr_changed:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = 0.005\n",
    "            \n",
    "            lr_changed = True\n",
    "        \n",
    "        if loss < 0.035 and not lr_change2:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = 0.0008\n",
    "                \n",
    "            lr_change2 = True\n",
    "\n",
    "\n",
    "        val_loss = eval_performance(model, val_loader)\n",
    "        val_loss_hist.append(val_loss)\n",
    "\n",
    "        smallest_val_loss = min(smallest_val_loss, val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch + 1} of {NUM_EPOCHS}, Train Loss: {loss}, Val Loss: {val_loss}')\n",
    "        # print(f'Epoch {epoch + 1} of {NUM_EPOCHS}, Train Loss: {loss}')\n",
    "\n",
    "\n",
    "        patience = 30\n",
    "        # Decide if early stopping necessary \n",
    "        recent_min = min(val_loss_hist[-patience:])\n",
    "        if smallest_val_loss < recent_min:\n",
    "            print(\"Stopped early\")\n",
    "            return\n",
    "\n",
    "        # # Saving a full batch!\n",
    "        # if epoch % 2 == 0:\n",
    "        #     save_img(y_vals.cpu().data, name='./Cone_Images/truth_batch_{}.png'.format(epoch))\n",
    "        #     save_img(outputs.cpu().data, name='./Cone_Images/predicted_batch_{}.png'.format(epoch))\n",
    "        \n",
    "        epoch += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/h7/94j8y0qs6r76hpyncbj21th40000gn/T/ipykernel_30671/3655824926.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/h7/94j8y0qs6r76hpyncbj21th40000gn/T/ipykernel_30671/3455131070.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, NUM_EPOCHS)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Calculate model outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m             \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn_response/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/h7/94j8y0qs6r76hpyncbj21th40000gn/T/ipykernel_30671/2786339656.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;31m# print(x.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# print(x.detach().numpy().shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn_response/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1188\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1189\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1190\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1191\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn_response/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/nn_response/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    456\u001b[0m             return F.conv2d(F.pad(input, self._reversed_padding_repeated_twice, mode=self.padding_mode),\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m                             _pair(0), self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train(model, train_loader, NUM_EPOCHS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"training loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"train loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(train_loss_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"validation loss history\")\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"val loss\")\n",
    "plt.yscale(\"log\")\n",
    "plt.plot(val_loss_hist[:])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Show a sample output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def display_sample(model, data_loader):\n",
    "    for x, y in data_loader:\n",
    "        # Set model to evaluation mode to conserve memory \n",
    "        model.eval()\n",
    "\n",
    "        # Don't want to waste memory on gradients\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = model(x)\n",
    "\n",
    "            truth = torch.reshape(y[0], (DEPTH, NUMPIX, ))[3].detach().numpy()\n",
    "            model_pred = np.reshape(pred[0].detach().numpy(), (DEPTH, NUMPIX, ))[3]\n",
    "\n",
    "            diff = np.absolute(model_pred - truth)\n",
    "            \n",
    "            # [0] because 1 channel so need to go inside\n",
    "            hp.mollview(truth, title=\"Ground Truth\", nlocs=5)\n",
    "            hp.mollview(model_pred, title=\"Model Prediction\", nlocs=5)\n",
    "            hp.mollview(diff, title=\"Difference Map\", nlocs=5)\n",
    "\n",
    "            print(np.mean(diff))\n",
    "\n",
    "            # set back to train mode\n",
    "            model.train()\n",
    "            break\n",
    "    \n",
    "if SHOW_IMAGES:\n",
    "    display_sample(model, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Val loss: \" , eval_performance(model, val_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dataloader of Model Outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_outputs(model, path):\n",
    "    preds = []\n",
    "    truths = []\n",
    "\n",
    "    for x, y in val_loader:\n",
    "    # Set model to evaluation mode to conserve memory \n",
    "        model.eval()\n",
    "\n",
    "        # Don't want to waste memory on gradients\n",
    "        with torch.no_grad():\n",
    "\n",
    "            pred = model(x)\n",
    "\n",
    "            pred_np = pred.detach().numpy()\n",
    "            truth_np = y.detach().numpy()\n",
    "\n",
    "            new_shape = (4, 12, 64)\n",
    "\n",
    "            # batched right now, so need to unbatch \n",
    "            for i in range(len(pred_np)):\n",
    "\n",
    "                # need to also reshape each element\n",
    "                pred_reshape = np.reshape(pred_np[i], new_shape)\n",
    "                truth_reshape = np.reshape(truth_np[i], new_shape)\n",
    "\n",
    "                print(pred_reshape.shape)\n",
    "\n",
    "                # need to add each depth individually\n",
    "                for depth in range(4):\n",
    "                    # [0] since first 1 dimension\n",
    "                    preds.append(pred_reshape[depth])\n",
    "                    truths.append(truth_reshape[depth])\n",
    "    \n",
    "    preds = np.array(preds)\n",
    "    truths = np.array(truths)\n",
    "\n",
    "    print(preds.shape)\n",
    "            \n",
    "    combined_set = TensorDataset(torch.tensor(preds).to(dtype=base, device=device), torch.tensor(truths).to(dtype=base, device=device))\n",
    "    loader = DataLoader(combined_set, 1, shuffle=True)\n",
    "    \n",
    "    torch.save(loader, path)\n",
    "\n",
    "    # set back to train mode\n",
    "    model.train()\n",
    "\n",
    "final_train_loss = round(float(train_loss_hist[-1]), 3)\n",
    "final_val_loss = round(float(val_loss_hist[-1]), 3)\n",
    "\n",
    "name = f\"saved-pred-outputs/output-loader-model_NSIDE{NSIDE}_trainloss{final_train_loss}_valloss{final_val_loss}_epochs{epoch}.pth\"\n",
    "save_model_outputs(model, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('nn_response')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a63c20d5ea98d5ea615593d44a9fb564882bf3b15df23c63fb54c70ca4209996"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
